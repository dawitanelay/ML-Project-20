{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split \n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras import Sequential\n",
    "from keras import regularizers\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# for combining all the hyper-parameters\n",
    "import itertools \n",
    "from time import time\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Grid search for the best parameter\n",
    "g_eta = [0.1]\n",
    "g_batchSize = [20,40]\n",
    "g_hiddeLayerunit1 = [4,8]\n",
    "g_momentum = [0.8,0.9]\n",
    "g_afHiddenLayerunit1 = [\"relu\", \"tanh\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# reading training and testing data from the csv file\n",
    "trainDf = pd.read_csv ('monks-1-test.csv',header=None).to_numpy()\n",
    "testDf = pd.read_csv ('monks-1-train.csv',header=None).to_numpy()\n",
    "trainDf = minmax_scale(trainDf, feature_range=(0,1), axis=0)\n",
    "testDf = minmax_scale(trainDf, feature_range=(0,1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Preparing trainDf for training\n",
    "trainX = trainDf[:, 1:7]\n",
    "trainY = trainDf[:, 0]\n",
    "#trainX = scaler.fit_transform(trainX)\n",
    "#trainY = scaler.fit_transform(trainY)\n",
    "\n",
    "#Preparing testDf for validation\n",
    "testX = testDf[:, 1:7]\n",
    "testY = testDf[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # one hot encode\n",
    "# train_one_hot = to_categorical(trainX,num_classes = 5)\n",
    "# # invert encoding\n",
    "# inverted = argmax(encoded[5:6])\n",
    "# print(inverted)\n",
    "# train_one_hot[5:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_model(lr=0.1,momentum=0.9,g_hiddeLayerunit1=4,g_hiddeLayerunit2=4,activation1=\"tanh\",activation2=\"tanh\",activation3=\"sigmoid\",init_mode='uniform', g_decay=0.01):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(g_hiddeLayerunit1, input_dim=6, kernel_initializer=init_mode, activation=activation1))\n",
    "    model.add(Dropout(0.4)),\n",
    "    model.add(Dense(g_hiddeLayerunit2, kernel_initializer=init_mode, activation=activation2,kernel_regularizer=regularizers.l1(0.01))) # l1 regularization parameter could also be something we can also grid search for.\n",
    "    model.add(Dropout(0.4)),\n",
    "    model.add(Dense(1, activation3))\n",
    "    sgd = SGD(lr=lr, momentum=momentum, decay=g_decay, nesterov=False,)  # We can add decay to hyper parameter list to get optimum value. \n",
    "    model.compile(optimizer=sgd, loss='mean_squared_error',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3456 candidates, totalling 10368 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   13.5s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   45.6s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  2.2min\n",
      "/home/anelay/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  7.4min\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "start=time()\n",
    "model = KerasClassifier(build_fn=create_model)\n",
    "\n",
    "# define the grid search parameters\n",
    "batch_size = [20,40]\n",
    "epochs = [100]\n",
    "g_hiddeLayerunit1=[4,8]\n",
    "g_hiddeLayerunit2=[4,8]\n",
    "g_decay = [0.01] # more estimations can be added\n",
    "activation1 = [\"relu\", \"sigmoid\",\"softmax\"]\n",
    "activation2 = [\"relu\",\"sigmoid\",\"softmax\"]\n",
    "activation3 = [\"softmax\"]\n",
    "init_mode = ['uniform','glorot_uniform', 'he_normal']\n",
    "lr=np.arange(0.1, 0.9, 0.1).tolist()\n",
    "momentum=np.arange(0.1, 0.9, 0.1).tolist()\n",
    "param_grid = dict(\n",
    "    lr=lr,\n",
    "    g_hiddeLayerunit1=g_hiddeLayerunit1,\n",
    "    g_hiddeLayerunit2=g_hiddeLayerunit2,\n",
    "    activation1=activation1,\n",
    "    activation2=activation2,\n",
    "    activation3=activation3,\n",
    "    batch_size=batch_size,\n",
    "    init_mode=init_mode,\n",
    "    g_decay = g_decay,\n",
    "    epochs=epochs)\n",
    "\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid,n_jobs=-1,cv=3,verbose=2)\n",
    "\n",
    "\n",
    "\n",
    "grid_result = grid.fit(trainX, trainY)\n",
    "\n",
    "end=time()\n",
    "\n",
    "print(\"Total Running Time: %f\",end-start)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "print(\"Mean\\tSTD\\tParams\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StratifiedKFold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c078353f5a0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mestimators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mlp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKerasClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_larger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mkfold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Larger: %.2f%% (%.2f%%)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'StratifiedKFold' is not defined"
     ]
    }
   ],
   "source": [
    "# model for monke1\n",
    "\n",
    "def create_larger():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_dim=6, activation='relu'))\n",
    "    model.add(Dense(25, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_larger, epochs=100, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
    "print(\"Larger: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
